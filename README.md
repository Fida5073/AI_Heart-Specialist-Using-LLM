# AI_Heart-Specialist-Using- Deep Seek LLM Model
This Project is designed to utilize an advanced language model for providing cardiovascular diagnostics and treatment recommendations based on input medical questions. Here's a breakdown of the code in the present tense:

Setup and Installation: The code begins by installing necessary libraries. It installs the latest version of the unsloth package, which is used for setting up the language model, and verifies if Hugging Face’s API token is correctly set up.

Importing Libraries: Key libraries such as FastLanguageModel from unsloth, torch for GPU support, transformers for model interaction, and other necessary modules are imported. These libraries help in setting up the model and managing datasets.

Login and GPU Check: The code logs into Hugging Face using the stored API token (hf_token) and checks whether CUDA (GPU support) is available to ensure that the model runs efficiently on the GPU. It prints out whether CUDA is available and identifies the GPU model if available.

Model Setup: The code installs transformers and bitsandbytes to load and configure the pre-trained heart-specialized model. The model (deepseek-llm-7b-chat) is loaded using the AutoModelForCausalLM class, and BitsAndBytesConfig is used for 4-bit quantization, making the model run more efficiently.

System Prompt for Heart Specialist: A prompt is set up specifically for a heart specialist. The system prompt instructs the model to answer heart-related medical queries with advanced knowledge in cardiovascular diagnostics and treatment planning. This prompt ensures the model understands the context and the type of questions it will be answering.

Defining a Test Question: A heart-related question is defined to test the model. The question involves a 61-year-old woman with symptoms such as chest pain, shortness of breath, and fatigue, and a history of hypertension. The question asks what diagnostic tests would be appropriate to evaluate her condition and potential heart failure.

Tokenizing the Input: The test question is tokenized using the model’s tokenizer, which prepares the input for the language model to process. The tokenized input is then passed to the GPU for efficient processing.

Generating the Response: The model generates a response based on the input question using its pre-trained knowledge. It does this by running the generate() method, which generates the most relevant and coherent answer to the query. The response is generated with a maximum of 1500 new tokens.

Decoding and Displaying the Answer: The output tokens generated by the model are decoded back into human-readable text. The code prints the full response and extracts the part of the response that comes after the phrase "### Answer:", which contains the model's actual answer to the query.
